Design
======

This paradigm of design is split into specification side and implementation side, each of which are decomposed into simpler
paradigms.

The inspiration for this paradigm is that of mathematically defined spaces: Each such space is given a constructive grammar
(say R^2 which uses the constructive subgrammars of R, Q, Z, N, along with the product verb, etc). Once the space is constructed,
the grammars along with basic (usually monoidal) functions will offer us ways to navigate these spaces. From there, we privilege
spaces which possess a multitude of easily definable subspaces and subnavigators as they allow us to model many applications. 
Such spaces are thus called *navigably complex*. Subspaces are structural types. Any subspace as model requires us to reinterpret
that subspace by means of functional types. Thus, using predicate logic, we create an algebra of filters (dual concept of a type)
where we can interpret and reinterpret the *type* of the subspace, equipping and reequipping it with alternative interfaces.
Functional typology is perspective of type.

As for predicate logic, it is entirely based on relationships of comparisons of similarity. There are three main ways to
view similarity: identity, proximity, behaviour. Identity is in equality or equivalance. Proximity is in order which determines
nearness. Behaviour is in function which preserves mappings.

Specification side:

1. narrative interface design: A polymorphic space is narratively constructed through the use of template definitions as generic
combinatorial specifications. As the full space has at least a partially described structure, we can then specify subspaces
by declaring substructures, how to compress their representations, and how to navigate these compressions relative to each other.
2. with these compressed navigable subspaces as our interfaces, we are able to additively extend the filter, or dually,
restrict the interface as type. In doing so we have access to the navigators of the extended filter or restricted type. 
3. given the nature of the language we're applying this design to, we take advantage of its natural efficiencies and optimizations
and dispatch partial specializations which fit the front end interface but might differ from the generic internals for optimization
purposes.

Implementation side:

1. we modularize our narratives by tethering them to hardware abstractions. For each hardware module, we use the constructive
narrative to further modularize any given narrative path into constructive objects.
2. classify the code for each object as structure, navigation, filters, instances (optimizations).
3. the filters and instances have both semiotic and media versions: Semiotic code has requirement to be elegant, efficient,
reusable, but otherwise has no requirement to be safe. Media code has requirement to be safe, and is usually just a safe
version (built from the semiotic code) for general users.
4. instances are the partial specializations that match efficiencies of the language with the narrative interface design.

----------------------------------------------

How to compress a long string of instructions:
----------------------------------------------

We privilege the idea of function composition.

When looking at a list of code, we want to partition it with the goal of maximally compressing
it---replacing the partition classes with shortnames.

The most ideal pattern is to find a block in which are recognized partitions are dependent on
each other in a linear or axiomatic fashion---this is to say the partitions are not independent
of each other. We may then recursively compress such code.

In theory you can take variations on the idea of recursively compressible code, but you always
have to weigh it against its computational efficiency, as well as its user-intuitive appeal.
Something might be highly compressible but not at all intuitive for a human coder.

Beyond that, we then look at partitions of code which the classes become increasingly independent
of each other. The other extreme of the spectrum are true code partitions---completely independent classes.

When looking for independent partitions, the algorithm is as follows:

1) Split the instruction set into parts by first searching for recursively compressible code.
2) Of the non-recursively compressible code, take the first encountered, and compare it against the rest
   to see if they have any sublists in common (in set-theoretic terms, take the intersection).
   This way the parts are grinding against each other splitting each other up into finer parts (synchronization?).
3) Refactor the common parts, and repeat until the whole list is complete.
4) An optimization of this approach is to search for dependent names, and greedily grow the largest subgroups
   to separate out functions. This approach allows you then to scope and lexically rename variables to continue
   finding repeating subpatterns.

classes are considered variant types if they share the same structural base. For example the "identity" vector as well as the "uinteger"
would share the same base, and so are both under the general type "vector". This includes for example the iterator subtypes
belonging to the "iterator" type, though this is a technicality as their respective bases are specializations of a broader template.

"include" dependency rules:

The context is what matters, but it's the general idea that the media space is ultimately more similar to the context space
than the semiotic space that drives the plot. The media space is a bottom-up representation of a top-down context space.
We do our best to build the media space as a mathematically closed implementation as close as possible to the context specification.
A mathematically closed system is obvious preferred as it is less error/contradiction prone. As it is bottom-up we build breadth
then depth of concepts ever abstracting to higher constructs.

We privilege context over media, and media over semiotics. The filesystem layout of directories is our contextual specification
of type. Any files thus are implementation and thus media oriented, but as efficiency is still a key concern, we also have the
possibility of efficient but unsafe (mathematically incomplete) versions of the media code.

As for dependencies:

As we build outward and upward higher constructs depend on simpler ones. There should be no dependency resolution issues from
one level of abstraction to a higher level of abstraction. As for code at the same level: Different contextual types are expected
to maintain the narrative of increasing complexity and thus there should be no issue there. Finally, for a give namespace type,
you will have both semiotic as well as media implementations. From a narrative perspective, media is built out of semiotics when
both exist, and so media files should include semiotic files. In the case that some semiotic code relies on media code, a simple
declaration of the media within the semiotics will suffice.

As for shortnames:

abc\_name\_def

The "abc" part is the first letter of each nested namespace until the final namespace which is represented by name.
Thus the "abc\_name" part of this string are specification side names, while the "def" part are implementation side names.
The "\_name\_" part thus signals the barrier between specification and implementation.

# Context Space

The library design as a whole primarily extends the existing grammar of the existing C++ language---as it is implementation oriented.
We could do that for each modular and/or extensible builtin grammar point (and we do), but we'd still be left with a semantic mess
of how to organize it all after that. We categorize the grammar points as a *technology space*.

Our technology space is a computability space. A computability space's context is a memory space. Looking at the grammar space of C++,
the memory space is oriented around the memory hardware as well as the primitive builtin types. Extending this, its type system
is further built on top of templates. Thus, any classes that maximally privilege the representation of hardware memory, primitive types,
as well as templates should be centered in the context space. Breaking down the context space though, the most abstract or low level
interactions with types and hardware is the context of the context space. The base form of hardware memory is the register, so any
code which idealizes that belongs in the context. The semiotics have more to do with refined constructs to represent more refined
forms of memory such as the RAM. Thus, generic high entropy highly compressible constructs (templated iterator and pointer methods)
belong here. In the context's media space belong less compressible safer forms of meaningful generic algorithms.

Decomposed as its own technology space:

## Pedagogical Narrative:

The main question to keep in mind when interpreting the context is: What "universal" approach do we take to navigating memory?

As our technology space has been narrowed to a *computability space*, the context is interpreted as *memory* in an abstract sense.
The other major theme is that the context as implementation is where we store our generic templated methods. As this is the case,
our *template space* context becomes metacode: template code as overhead to ease the use of template programming toward our intended
generic memory methods. Templates (within the C++ grammar) are the language we use to access memory in the most general and universal way.
Regardless of how we represent our primitives of templates, and of how we constrain our use of them, they are our language of abstract
memory.

The template semiotic space---as computability space---is a *process* space: It holds primitive generic methods
(largely having to do with manipulating processor register data) used to represent access to memory in instantiated ways.

Our template media space then as constraint, becomes the constraints we allow in navigating memory. The thing is, there's no universal
best way to interpret what memory is, and so there ends up being more than one paradigm of navigation. The navigational constraint
made famous by the C++ STL is that of *iterators*. This is developed here certainly, but there is an alternative within the nik library:
*seperators*. Seperators are fundamentally sifters and filters when interpreting memory from the lens of set theory.

Template loop unrolling requires unambiguous policy as there are alternatives to general implementations.

Be it componentwise or arithmetic or general recursive, the solution arises from refactoring/compressing code to have a minimal footprint.
I have decided to borrow from mathematical/combinatorial best practices by making use of the identity value (zero value) as default
for the specialized initial "seed" code regarding unrolling. This would refactor the loop iterative code to be defined solely within
the general definition leaving the specialized case to be minimalistic as possible. Any "natural" (intuitive) implementations
as part of the user-friendly interface versions (as part of the clean/unsafe paradigm split) would handle any specialized
overhead.

There are several reasons for choosing this particular policy, though the main reason is in privileging the paradigm of seperating
clean "normal forms" from local user-friendly versions. If these unrolled loops really are cleanly defined, there's no need for
variant specializations in the first place. Having variant specializations would further encourage such poor/bad coding practices.

The other consideration is when one is able to define composite carrys being passed along the unrolled loop.
If one has a choice between extending the modification within the body before the next iterative call,
or within the arguements passed within the next iterative call, for code readability at least it is better to do so within the body.

I have run some basic g++ compiler behavioural tests suggesting an important property: Incrementation under template unrolling
is not persistent. As such, in order to be able to specialize within the user-friendly versions, one has two options:
Use the return version of unrolling to update the iterator, or pass the iterator type as a reference when calling.

How a #Mathematician sees #ComputingScience:

With implementation, you should have a universal medium (a universal construct language), then you can build non-linear stories
around an efficient implementation (minimizing redundancy and overhead) and then simply navigate the story-specifications in more
complex ways. This is already done with the relationship between hard drive storage and filesystems. Hard drive is (close) to universal,
while the filesystem is lightweight stories. This is already done in math: The real line (or n-dimensional real number system) is universal,
while stories around it become nuanced and are in need of localized navigational semiotics and semantics.

Policy: 
	Other parts of the library simply assume "full_register" as default if not explicitly named.
	That is the general policy (lazy modularization), but here the context is too broad to simply assume.

# Semiotic

The semiotic space is for algorithms (methods; functions; procedures) which are inherently unsafe but otherwise very useful.

General rule of thumb is if it isn't a grammatically clean recursive form, it doesn't belong here.
If it's a body of code that shows up a lot and so is refactored, that's more of a user-friendly safety than anything,
and thus it likely belongs in the context::media space. The exception to the rule is if it is a semantically meaningful
non-recursive form which is unsafe, then it might be best categorized here.

If it is grammatically clean recursive but otherwise unsafe or not-necessarily-safe then it should be here.

# Media Space

Repository of safe classes and procedures.

How do you define safe? There's no way to semantically evaluate the code (to assume it does what's intended),
so instead one looks to make sure it's syntactically safe. This is to say it won't crash on any input,
as well, it has *safeties* which is to say all input is actual content, not overhead input.

Decomposed as its own technology space:

+ context - *literic*.
+ semiotic - *graphic*, *interic*, *kinetic*, *numeric*, *phonetic*.
+ media - *generic*.

## Intuition

Borrowing from the *Unix Philosophy*:

> Write programs that do one thing and do it well.
> Write programs to work together. Write programs to handle text streams,
> because that is a universal interface. (A Quarter Century of Unix, Salus)

Textual strings are universal. This is to say, the hardware type which has highest entropy (information theory)
relative to the other hardware representations is the literic classes which deal with textual and computational
literary analyses. It follows then that this would be the *subtext* (sub-context space) within the decomposition
of the main media space.

Regarding subsemiotics: The graphic, interic, kinetic, numeric, and phonetic classes are all intended to represent
a specific hardware component, and as such fit best into the semiotic framework. The *numeric* classes are notably
privileged with much time spent in optimization as they primarily consist of extended number classes: *block* being
a templated *int* class of dynamic but fixed length.  The *dynamic_block* is like the "Big Int" class in the Python
or Java programming languages. As arithmetic is used repeatedly in many different classes throughout this library,
special attention has been paid to these "workhorse" (or in the Inuit case, "workdog") classes and should be for any
future extensions.

Finally, the submedia is represented with the generic but safe classes. Generics are generic data structures, which
are constraints applied to accessing the relationships between the subsemiotics and their subtext. In practice,
the relationship between signifier and signfied (context) plays out as *translation*. As strings are the universal
medium, we should be able to translate any of the other specific hardware representations to and from string data.
In this sense the other classes are centralized around the literics classes. In particular then, special care is also
given to some of these classes like *regular expressions* because they also form workhorse classes within this whole
library.

## Folders

The current folders within this directory are as follows:

+ generic
+ graphic
+ interic
+ kinetic
+ literic
+ numeric
+ phonetic

## Pedagogical Narrative:

Logically it makes sense to start with the *display* class within the *generic* folder; it is the STL equivalent of *cout*.
This is necessary as debugging is constantly required for every single class added to this library, and one of
the best ways to do so is to print values to the screen.

